---
title: "Modeling Household Electricity Demand"
author: "Xin Guan, Vera Hudak, Yuqi Zhang"
date: "2024-05-24"
output: pdf_document
---
```{r}
library(electBook)
library(dplyr)
library(tidyr)
library(lubridate)
library(proxy)
library(tibble)
library(ggplot2)
library(bookdown)
library(rjags)
```


## Introduction

Electricity demand forecasting is crucial for energy management, planning, and policy-making. In this report, we explore advanced statistical techniques to model and predict household electricity demand using a dataset of Irish residential consumers. The dataset, provided by the CER Smart Metering Project, includes half-hourly electricity consumption data and survey information from over 2000 households. Given the extensive nature of the data, a primary challenge is to effectively aggregate and analyse it to extract meaningful insights and make accurate predictions.

Our approach involves the following steps:

1. **Exploratory Data Analysis (EDA):** We begin with a thorough exploratory analysis of the dataset to understand the distribution and variability of electricity demand across different households and identify key patterns and trends. This step helps in detecting outliers, understanding the data structure, and selecting relevant features for modelling.

2. **Aggregation:** To manage the large dataset, we use an aggregation method to group households based on daily demand profile similarity. This step reduces the dimensionality of the data and enables us to focus on representative groups, making the modelling process more efficient.

3. **Prediction:** We develop a Bayesian time series regression model that incorporates both temporal dependencies and non-linear relationships between electricity demand and explanatory variables. Bayesian methods offer a robust framework for parameter estimation and uncertainty quantification, providing comprehensive insights into demand patterns.

4. **Evaluation:** The predictive performance of our model is evaluated using appropriate metrics to ensure its accuracy and reliability. We compare the predicted values against the actual data to assess the model's effectiveness in capturing the underlying demand patterns.

This report aims to demonstrate the application of advanced Bayesian techniques in electricity demand forecasting and highlight the benefits of data aggregation in handling large datasets. By leveraging Bayesian methods, we aim to contribute to the development of more adaptive and reliable forecasting models that can meet the evolving challenges of more complicated dataset.


## Explanatory Data Analysis

Loading Data:
```{r}
load("Irish.RData")
```

The file `Irish` contains three data sets, the data set`Irish$indCons` is the electricity demand for 2671 household, with a resolution of half hour over the time period 2019-12-29 to 2020-12-29.
```{r}
head(Irish$indCons[,1:10])
```
The data set `Irish$extra` contains variables such as `tod` (time of day), `toy` (time of year), `dow` (day of the week), and more.
```{r}
head(Irish$extra)
```
The data set `Irish$survey` contains responses from a household survey, including variables such as SOCIALCLASS, OWNERSHIP, BUILT.YEAR, and others. The variables in this data frame doesn't show any significant relationship with demand, hence we will not use this data in our project.
```{r}
head(Irish$survey)
```

### Count zeros in dataset
```{r}
sum(apply(Irish$indCons, 2, function(x) sum(x == 0)))
print(16799*2674)
```

```{r}
row_zero_counts <- rowSums(Irish$indCons == 0)

hist(row_zero_counts)
```
There are more than 17,000 zero values in the dataset. Summing over the rows for each time point, there is no any time point have significantly large number of zeros.
```{r}
# Count zeros in each column
col_zero_counts <- colSums(Irish$indCons == 0)

# Histogram of columns with more than 100 zeros
hist(col_zero_counts[col_zero_counts > 100], 
     main="Histogram of Households with More Than 100 Zero Usage", 
     xlab= "Zero Counts per Household")
```

```{r}
sum(col_zero_counts > 30*48)
```

```{r}
cols_to_remove <- which(col_zero_counts > 30*48)

df0 <- Irish$indCons[,-cols_to_remove]

df0$time_mean_dem <- rowSums(Irish$indCons)/ncol(Irish$indCons)
```

By examining the histogram of zero counts for each household below, we found that some households had zero electricity usage for over one month during the year. Therefore, we decided to remove these 27 households from our dataset.

### Visualizing main characteristics
Compute the mean demand for each time point, for visualization and examining pattern.
```{r}
df <- cbind(df0[,"time_mean_dem"],Irish$extra)
colnames(df) <- c("time_mean_demand", colnames(Irish$extra))
head(df)
```

```{r}
# Basic summary of each column
summary(df)
```
The variable `holy` is all FALSE, we will remove this variable.

```{r}
# Time series plot of time_mean_demand
ggplot(df, aes(x=dateTime, y=time_mean_demand)) + geom_line() + 
  ggtitle("Time Series of Mean Demand")
```
Plot the time series of mean daily demand throughout the year. The plot presents a periodic pattern of electricity demand, with higher demand during the winter months and lower demand during the summer months.

```{r}
# Boxplots to check variation of time_mean_demand across days of the week
ggplot(df, aes(x=dow, y=time_mean_demand)) + geom_boxplot() + 
  ggtitle("Demand Variation by Day of Week")
```
The box plots of mean demand for each day of the week shows that the demand patterns are relatively consistent across the weekdays, with slight variations during the weekends. The box plots reveal that weekends tend to have slightly higher demand compared to weekdays.

```{r}
# Scatter plot of time_mean_demand vs. temperature
ggplot(df, aes(x=temp, y=time_mean_demand)) +
    geom_point(alpha=0.5) +
    geom_smooth(method="lm", se=FALSE, color="blue") +
    labs(x="Temperature", y="Mean Demand", 
         title="Relationship Between Temperature and Mean Demand")
```
The above plot explores the relationship between temperature and mean demand. The scatter plot suggests an inverse relationship, where higher temperatures generally correlate with lower electricity demand.

```{r}
# Line plot for time_mean_demand across different times of day
ggplot(df, aes(x=tod, y=time_mean_demand, group=1)) +
    geom_point(alpha=0.5) +
    geom_smooth(color="blue") +
    labs(x="Time of Day", y="Sum Demand", 
         title="Mean Demand Across Different Times of Day")

```
The plot above illustrates the mean demand across different times of the day. The plot shows a
clear pattern, with demand peaking in the early morning, around noon, and in the evening, likely
reflecting typical household activity patterns.

## Aggregation

### Formatting the data for clustering
```{r}
# Load the dataset
data(Irish)

# Calculate the number of zero values in each column
col_zero_counts <- colSums(Irish$indCons == 0)

# Identify columns to remove (columns with more than 30*48 zero values)
cols_to_remove <- which(col_zero_counts > 30 * 48)

# Create a data frame with demand data and remove identified columns
df <- Irish$indCons
df <- df[,-cols_to_remove]

# Add date and time columns
df$date <- as.Date(Irish$extra$dateTime)
df$time <- format(Irish$extra$dateTime, "%H:%M:%S")

# Gather the data into long format
df_long <- df %>%
  pivot_longer(cols = -c(date, time), names_to = "household_id", 
               values_to = "demand")

# Calculate the average demand over the year for each 30-minute interval
avg_demand <- df_long %>%
  group_by(household_id, time) %>%
  summarise(average_demand = mean(demand, na.rm = TRUE)) %>%
  ungroup()

avg_demand_wide <- avg_demand %>%
  pivot_wider(names_from = time, values_from = average_demand)

# Display the result
print(avg_demand_wide)
```

### Compute cosine similarity and corresponding distance
```{r}
# Compute the cosine similarity, and use 1-similarity as distance
compute_cosine_distance_matrix <- function(data) {
  data_matrix <- as.matrix(data[-1])  # Remove the household_id column
  similarity_matrix <- proxy::simil(data_matrix, method = "cosine")
  dist_matrix <- 1 - similarity_matrix
  return(as.matrix(dist_matrix))
}

cosine_distances <- compute_cosine_distance_matrix(avg_demand_wide)
```

### Perform hierarchical slustering
```{r}
# Hierarchical clustering
hc <- hclust(as.dist(cosine_distances), method = "ward.D2")

# Plot the dendrogram
plot(hc, labels = FALSE, main = "Dendrogram of Households", xlab = "Households", 
     ylab = "Height")
abline(h=1,col="red")
```

### Clusters information
```{r}
# Create clusters
clusters <- cutree(hc, k = 6)
avg_demand_wide$cluster <- clusters
# Summarize the number of households in each cluster
cluster_summary <- avg_demand_wide %>%
  group_by(cluster) %>%
  summarise(num_households = n())

# Display the summary
print(cluster_summary)

# Reshape avg_demand back to long format
avg_demand_long <- avg_demand_wide %>%
  pivot_longer(cols = -c(household_id, cluster), names_to = "time", 
               values_to = "daily_demand")


# Join cluster information back to the original dataframe
df_with_clusters <- df_long %>%
  left_join(avg_demand_long, by = c("household_id", "time"))

# Analyze cluster characteristics
cluster_analysis <- df_with_clusters %>%
  group_by(cluster) %>%
  summarise(
    average_demand = mean(daily_demand, na.rm = TRUE)
  )

print(cluster_analysis)
```

### Aggregate data for households in the same cluster
```{r}
df_t <- as.data.frame(t(df[,-c(ncol(df)-1,ncol(df))]))

# Step 2: Add the clusters as a new column to the transposed data frame
df_t$cluster <- clusters

# Step 3: Group by cluster and calculate the mean for each row within each 
#cluster
mean_by_cluster <- df_t %>%
  group_by(cluster) %>%
  summarise(across(everything(), mean, na.rm = TRUE))

mean_by_cluster <- as.data.frame(mean_by_cluster[,-1])
rownames(mean_by_cluster) <- c("Cluster 1","Cluster 2","Cluster 3","Cluster 4",
                               "Cluster 5","Cluster 6")
# View the result
head(mean_by_cluster[,1:10])
```

```{r}
df0 <- Irish$extra
df0 <- df0 %>% mutate(dow = ifelse(dow %in% c("Sat", "Sun"), "True", "False")) %>% select(-c(holy,time,dateTime))%>%rename(weekend = dow)
df0 <-t(df0)
```

```{r}
colnames(df0) <- colnames(mean_by_cluster)
mean_by_cluster <- rbind(mean_by_cluster,df0)
mean_by_cluster <- as.data.frame(t(mean_by_cluster))
# Convert Cluster  and temp columns to numeric, weekend into logic
mean_by_cluster <- mean_by_cluster %>%
  mutate(across(starts_with("Cluster"), as.numeric),
         weekend = as.logical(weekend),
         temp = as.numeric(temp))
```
The data is formatted and ready for model fitting, uncomment the following code to save data as CSV file/
```{r}
#write.csv(mean_by_cluster, file = "AggregatedData1.csv", row.names = FALSE)
```

```{r}
avg_demand_wide$cluster <- clusters
#write.csv(avg_demand_wide, file = "cluster_data.csv", row.names = FALSE)
```

### Plot the mean demand and sample demands for each cluster
In the following plots, we want to visually show if the clustering give a large between-clusters discrepancy and a small within-cluster discrepancy.
```{r}
# Identify the time columns by excluding non-time columns
time_columns <- grep("^\\d{2}:\\d{2}:\\d{2}$", colnames(avg_demand_wide), 
                     value = TRUE)

# Add rowid to avg_demand_wide for sampling
avg_demand_wide <- avg_demand_wide %>%
  mutate(rowid = row_number())

# Convert data to long format for plotting
avg_demand_long <- avg_demand_wide %>%
  pivot_longer(cols = all_of(time_columns), 
               names_to = "Time", 
               values_to = "Demand") %>%
  mutate(Time = as.numeric(gsub(":", "", Time)))

# Calculate mean demand for each cluster
mean_demand <- avg_demand_long %>%
  group_by(cluster, Time) %>%
  summarize(mean_demand = mean(Demand), .groups = 'drop')

# Plot mean demand for each cluster
for (cl in unique(avg_demand_wide$cluster)) {
  # Filter data for the cluster
  cluster_data <- avg_demand_long %>% filter(cluster == cl)
  
  # Sample 10 houses from the cluster
  sampled_houses <- sample(unique(cluster_data$rowid), 10)
  sample_data <- cluster_data %>% filter(rowid %in% sampled_houses)
  
  # Plot mean demand
  p <- ggplot() +
    geom_line(data = mean_demand %>% filter(cluster == cl), 
              aes(x = Time, y = mean_demand, color = "Mean Demand"), size = 1) +
    geom_line(data = sample_data, 
              aes(x = Time, y = Demand, group = rowid, 
                  color = as.factor(rowid)), alpha = 0.3) + 
    labs(title = paste("Cluster", cl),
         x = "Time",
         y = "Demand") +
    scale_color_manual(values = c("Mean Demand" = "red", setNames(rainbow(10), 
                                                            sampled_houses))) +
    theme_minimal()
  
  print(p)
}

```



## Linear Regression Model

### Data Preparation

We first prepare the data for each cluster.

```{r}
aggregated_data <- read.csv("AggregatedData1.csv")
daily_data <- read.csv("Daily_AggregatedData1.csv")

cluster1 <- aggregated_data[, -c(2,3,4,5,6)]
cluster2 <- aggregated_data[, -c(1,3,4,5,6)]
cluster3 <- aggregated_data[, -c(1,2,4,5,6)]
cluster4 <- aggregated_data[, -c(1,2,3,5,6)]
cluster5 <- aggregated_data[, -c(1,2,3,4,6)]
cluster6 <- aggregated_data[, -c(1,2,3,4,5)]

cluster1_trans <- cluster1 %>%
  mutate(Cluster.1_lag1 = lag(Cluster.1, n = 1)) %>%
  mutate(
    tod_poly1 = tod,
    tod_poly2 = tod^2,
    tod_poly3 = tod^3,
    tod_poly4 = tod^4,
    weekend_dummy = ifelse(weekend == "TRUE", 1, 0),
    toy_sin = sin(toy),
    toy_cos = cos(toy)
  ) %>%
  na.omit()

cluster2_trans <- cluster2 %>%
  mutate(Cluster.2_lag1 = lag(Cluster.2, n = 1)) %>%
  mutate(
    tod_poly1 = tod,
    tod_poly2 = tod^2,
    tod_poly3 = tod^3,
    tod_poly4 = tod^4,
    weekend_dummy = ifelse(weekend == "TRUE", 1, 0),
    toy_sin = sin(toy),
    toy_cos = cos(toy)
  ) %>%
  na.omit()

cluster3_trans <- cluster3 %>%
  mutate(Cluster.3_lag1 = lag(Cluster.3, n = 1)) %>%
  mutate(
    tod_poly1 = tod,
    tod_poly2 = tod^2,
    tod_poly3 = tod^3,
    tod_poly4 = tod^4,
    weekend_dummy = ifelse(weekend == "TRUE", 1, 0),
    toy_sin = sin(toy),
    toy_cos = cos(toy)
  ) %>%
  na.omit()


cluster4_trans <- cluster4 %>%
  mutate(Cluster.4_lag1 = lag(Cluster.4, n = 1)) %>%
  mutate(
    tod_poly1 = tod,
    tod_poly2 = tod^2,
    tod_poly3 = tod^3,
    tod_poly4 = tod^4,
    weekend_dummy = ifelse(weekend == "TRUE", 1, 0),
    toy_sin = sin(toy),
    toy_cos = cos(toy)
  ) %>%
  na.omit()

cluster5_trans <- cluster5 %>%
  mutate(Cluster.5_lag1 = lag(Cluster.5, n = 1)) %>%
  mutate(
    tod_poly1 = tod,
    tod_poly2 = tod^2,
    tod_poly3 = tod^3,
    tod_poly4 = tod^4,
    weekend_dummy = ifelse(weekend == "TRUE", 1, 0),
    toy_sin = sin(toy),
    toy_cos = cos(toy)
  ) %>%
  na.omit()

cluster6_trans <- cluster6 %>%
  mutate(Cluster.6_lag1 = lag(Cluster.6, n = 1)) %>%
  mutate(
    tod_poly1 = tod,
    tod_poly2 = tod^2,
    tod_poly3 = tod^3,
    tod_poly4 = tod^4,
    weekend_dummy = ifelse(weekend == "TRUE", 1, 0),
    toy_sin = sin(toy),
    toy_cos = cos(toy)
  ) %>%
  na.omit()
```

### Clusrer 1

**Linear Regression Model for Cluster 1**

On the basis of `cluster1_trans`, we fit a linear regression model and evaluate its performance:

```{r}
# Split the data into training and testing sets
train_index_1 <- 1:floor(0.8 * nrow(cluster1_trans))
train_data_1 <- cluster1_trans[train_index_1, ]
test_data_1 <- cluster1_trans[-train_index_1, ]

# Fit the linear regression model
linear_model_1 <- lm(Cluster.1 ~ Cluster.1_lag1 + temp + tod_poly1 + tod_poly2 
                     + tod_poly3 + tod_poly4 + weekend_dummy + toy_sin + 
                       toy_cos, data = train_data_1)
```

**Performance of Predictions**

```{r}
# Make predictions on the testing set
test_data_1$predictions <- predict(linear_model_1, newdata = test_data_1)

# Calculate prediction error metrics
mae_1 <- mean(abs(test_data_1$Cluster.1 - test_data_1$predictions))
mse_1 <- mean((test_data_1$Cluster.1 - test_data_1$predictions)^2)
rmse_1 <- sqrt(mse_1)

# Print the results
cat("Mean Absolute Error (MAE):", mae_1, "\n")
cat("Mean Squared Error (MSE):", mse_1, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_1, "\n")
```

```{r}
# Plot predicted vs true values
ggplot(test_data_1, aes(x = Cluster.1, y = predictions)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, color = 'red', linetype = "dashed") +
  labs(title = "Predicted vs True Values",
       x = "True Values",
       y = "Predicted Values") +
  theme_minimal()
```

**Linear Regression Model Summary**

```{r}
summary(linear_model_1)
```

### Cluster 2

**Linear Regression Model for Cluster 2**

```{r}
# Split the data into training and testing sets
train_index_2 <- 1:floor(0.8 * nrow(cluster2_trans))
train_data_2 <- cluster2_trans[train_index_2, ]
test_data_2 <- cluster2_trans[-train_index_2, ]

# Fit the linear regression model
linear_model_2 <- lm(Cluster.2 ~ Cluster.2_lag1 + temp + tod_poly1 + tod_poly2 
                     + tod_poly3 + tod_poly4 + weekend_dummy + toy_sin 
                     + toy_cos, data = train_data_2)
```

**Performance of Predictions**

```{r}
# Make predictions on the testing set
test_data_2$predictions <- predict(linear_model_2, newdata = test_data_2)

# Calculate prediction error metrics
mae_2 <- mean(abs(test_data_2$Cluster.2 - test_data_2$predictions))
mse_2 <- mean((test_data_2$Cluster.2 - test_data_2$predictions)^2)
rmse_2 <- sqrt(mse_2)

# Print the results
cat("Mean Absolute Error (MAE):", mae_2, "\n")
cat("Mean Squared Error (MSE):", mse_2, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_2, "\n")
```

```{r}
# Plot predicted vs true values
ggplot(test_data_2, aes(x = Cluster.2, y = predictions)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, color = 'red', linetype = "dashed") +
  labs(title = "Predicted vs True Values",
       x = "True Values",
       y = "Predicted Values") +
  theme_minimal()
```

**Linear Regression Model Summary**

```{r}
summary(linear_model_2)
```


### Cluster 3

**Linear Regression Model for Cluster 3**

```{r}
# Split the data into training and testing sets
train_index_3 <- 1:floor(0.8 * nrow(cluster3_trans))
train_data_3 <- cluster3_trans[train_index_3, ]
test_data_3 <- cluster3_trans[-train_index_3, ]

# Fit the linear regression model
linear_model_3 <- lm(Cluster.3 ~ Cluster.3_lag1 + temp + tod_poly1 + tod_poly2 
                     + tod_poly3 + tod_poly4 + weekend_dummy + toy_sin 
                     + toy_cos, data = train_data_3)
```

**Performance of Predictions**

```{r}
# Make predictions on the testing set
test_data_3$predictions <- predict(linear_model_3, newdata = test_data_3)

# Calculate prediction error metrics
mae_3 <- mean(abs(test_data_3$Cluster.3 - test_data_3$predictions))
mse_3 <- mean((test_data_3$Cluster.3 - test_data_3$predictions)^2)
rmse_3 <- sqrt(mse_3)

# Print the results
cat("Mean Absolute Error (MAE):", mae_3, "\n")
cat("Mean Squared Error (MSE):", mse_3, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_3, "\n")
```

```{r}
# Plot predicted vs true values
ggplot(test_data_3, aes(x = Cluster.3, y = predictions)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, color = 'red', linetype = "dashed") +
  labs(title = "Predicted vs True Values",
       x = "True Values",
       y = "Predicted Values") +
  theme_minimal()
```

**Linear Regression Model Summary**

```{r}
summary(linear_model_3)
```

### Cluster 4

**Linear Regression Model for Cluster 4**

```{r}
# Split the data into training and testing sets
train_index_4 <- 1:floor(0.8 * nrow(cluster4_trans))
train_data_4 <- cluster4_trans[train_index_4, ]
test_data_4 <- cluster4_trans[-train_index_4, ]

# Fit the linear regression model
linear_model_4 <- lm(Cluster.4 ~ Cluster.4_lag1 + temp + tod_poly1 + tod_poly2 
                     + tod_poly3 + tod_poly4 + weekend_dummy + toy_sin 
                     + toy_cos, data = train_data_4)

# Make predictions on the testing set
test_data_4$predictions <- predict(linear_model_4, newdata = test_data_4)

```

**Performance of Predictions**

```{r}
# Calculate prediction error metrics
mae_4 <- mean(abs(test_data_4$Cluster.4 - test_data_4$predictions))
mse_4 <- mean((test_data_4$Cluster.4 - test_data_4$predictions)^2)
rmse_4 <- sqrt(mse_4)

# Print the results
cat("Mean Absolute Error (MAE):", mae_4, "\n")
cat("Mean Squared Error (MSE):", mse_4, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_4, "\n")
```


```{r}
# Plot predicted vs true values
ggplot(test_data_4, aes(x = Cluster.4, y = predictions)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, color = 'red', linetype = "dashed") +
  labs(title = "Predicted vs True Values",
       x = "True Values",
       y = "Predicted Values") +
  theme_minimal()
```

**Linear Regression Model Summary**

```{r}
summary(linear_model_4)
```


### Cluster 5

**Linear Regression Model for Cluster 5**

```{r}
# Split the data into training and testing sets
train_index_5 <- 1:floor(0.8 * nrow(cluster5_trans))
train_data_5 <- cluster5_trans[train_index_5, ]
test_data_5 <- cluster5_trans[-train_index_5, ]

# Fit the linear regression model
linear_model_5 <- lm(Cluster.5 ~ Cluster.5_lag1 + temp + tod_poly1 + tod_poly2 
                     + tod_poly3 + tod_poly4 + weekend_dummy + toy_sin 
                     + toy_cos, data = train_data_5)
```

**Performance of Predictions**

```{r}
# Make predictions on the testing set
test_data_5$predictions <- predict(linear_model_5, newdata = test_data_5)

# Calculate prediction error metrics
mae_5 <- mean(abs(test_data_5$Cluster.5 - test_data_5$predictions))
mse_5 <- mean((test_data_5$Cluster.5 - test_data_5$predictions)^2)
rmse_5 <- sqrt(mse_5)

# Print the results
cat("Mean Absolute Error (MAE):", mae_5, "\n")
cat("Mean Squared Error (MSE):", mse_5, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_5, "\n")
```


```{r}
# Plot predicted vs true values
ggplot(test_data_5, aes(x = Cluster.5, y = predictions)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, color = 'red', linetype = "dashed") +
  labs(title = "Predicted vs True Values",
       x = "True Values",
       y = "Predicted Values") +
  theme_minimal()
```

**Linear Regression Model Summary**

```{r}
summary(linear_model_5)
```

### Cluster 6

**Linear Regression Model for Cluster 6**

```{r}
# Split the data into training and testing sets
train_index_6 <- 1:floor(0.8 * nrow(cluster6_trans))
train_data_6 <- cluster6_trans[train_index_6, ]
test_data_6 <- cluster6_trans[-train_index_6, ]

# Fit the linear regression model
linear_model_6 <- lm(Cluster.6 ~ Cluster.6_lag1 + temp + tod_poly1 + tod_poly2 
                     + tod_poly3 + tod_poly4 + weekend_dummy + toy_sin 
                     + toy_cos, data = train_data_6)
```

**Performance of Predictions**

```{r}
# Make predictions on the testing set
test_data_6$predictions <- predict(linear_model_6, newdata = test_data_6)

# Calculate prediction error metrics
mae_6 <- mean(abs(test_data_6$Cluster.6 - test_data_6$predictions))
mse_6 <- mean((test_data_6$Cluster.6 - test_data_6$predictions)^2)
rmse_6 <- sqrt(mse_6)

# Print the results
cat("Mean Absolute Error (MAE):", mae_6, "\n")
cat("Mean Squared Error (MSE):", mse_6, "\n")
cat("Root Mean Squared Error (RMSE):", rmse_6, "\n")
```

```{r}
# Plot predicted vs true values
ggplot(test_data_6, aes(x = Cluster.6, y = predictions)) +
  geom_point(color = 'blue', alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, color = 'red', linetype = "dashed") +
  labs(title = "Predicted vs True Values",
       x = "True Values",
       y = "Predicted Values") +
  theme_minimal()
```

**Linear Regression Model Summary**

```{r}
summary(linear_model_6)
```

### Conclusion for Linear Regression Models

```{r performance-table, results='asis'}
coefficients_1 <- summary(linear_model_1)$coefficients
coefficients_2 <- summary(linear_model_2)$coefficients
coefficients_3 <- summary(linear_model_3)$coefficients
coefficients_4 <- summary(linear_model_4)$coefficients
coefficients_5 <- summary(linear_model_5)$coefficients
coefficients_6 <- summary(linear_model_6)$coefficients

performance_1 <- data.frame(MAE = mae_1, MSE = mse_1, RMSE = rmse_1)
performance_2 <- data.frame(MAE = mae_2, MSE = mse_2, RMSE = rmse_2)
performance_3 <- data.frame(MAE = mae_3, MSE = mse_3, RMSE = rmse_3)
performance_4 <- data.frame(MAE = mae_4, MSE = mse_4, RMSE = rmse_4)
performance_5 <- data.frame(MAE = mae_5, MSE = mse_5, RMSE = rmse_5)
performance_6 <- data.frame(MAE = mae_6, MSE = mse_6, RMSE = rmse_6)

performance_summary <- rbind(
  data.frame(Cluster = "Cluster 1", performance_1),
  data.frame(Cluster = "Cluster 2", performance_2),
  data.frame(Cluster = "Cluster 3", performance_3),
  data.frame(Cluster = "Cluster 4", performance_4),
  data.frame(Cluster = "Cluster 5", performance_5),
  data.frame(Cluster = "Cluster 6", performance_6)
)

knitr::kable(performance_summary, 
             caption = "Performance Metrics for Each Cluster", 
             booktabs = TRUE)
```


```{r coefficients-table, results='asis'}
coef_summary <- data.frame(
  #Variable = rownames(coefficients_1),
  `Cluster 1` = coefficients_1[, "Estimate"],
  `Cluster 2` = coefficients_2[, "Estimate"],
  `Cluster 3` = coefficients_3[, "Estimate"],
  `Cluster 4` = coefficients_4[, "Estimate"],
  `Cluster 5` = coefficients_5[, "Estimate"],
  `Cluster 6` = coefficients_6[, "Estimate"]
)

knitr::kable(coef_summary, caption = "Coefficients Summary for Each Cluster", 
             booktabs = TRUE)
```

The tables referred to here as Table 1 and Table 2 provide a detailed overview of the initial performance metrics (using MAE (Mean Absolute Error), MSE (Mean Squared Error), and RMSE (Root Mean Squared Error)) and model coefficients for each cluster. These serve as a foundational baseline for assessing the effectiveness of the linear regression models. The primary purpose of these tables is to establish initial values and benchmarks for evaluating model performance. The insights gained from these metrics and coefficients are critical for understanding the model's predictive capabilities and identifying areas for potential improvement.

This foundational information will guide further analysis and model refinement, thereby enhancing our understanding of the underlying structure of the dataset and the effectiveness of the linear regression models.




## Bayesian Model

Load aggregated dataset:

```{r}
aggregated_data <- read.csv("AggregatedData1.csv")
```

First, we filter for cluster 1 data only from the aggregated dataset, and use the first 80% of it as training data, and the last 20 as the testing data. Note that we do this and not use random partitioning because that would disturb the time series nature of the data. 

```{r}
# use cluster 1 from dataset
cluster1_data <- aggregated_data[, -c(2,3,4,5,6)]

# take first 80% of the data
num_rows <- nrow(cluster1_data)
num_rows_80_percent <- floor(0.8 * num_rows)
cluster1_data_train <- cluster1_data[1:num_rows_80_percent, ]
cluster1_data_test <- cluster1_data[(num_rows_80_percent + 1):num_rows, ]

# define the dependent variable and teh explanatory variables
y <- cluster1_data_train$Cluster.1 
x4 <- cluster1_data_train$toy                    # time of year
x3 <- as.integer(cluster1_data_train$weekend)    # weekend
x1 <- cluster1_data_train$temp                   # temperature
x2 <- cluster1_data_train$tod                    # time of day
```

Next, we define our model. We assume that 

$$
    y_t \sim Normal (\mu_t, \sigma^2),
$$
where

$$
\mu_t = \beta_0 + \alpha y_{t-1} + \beta_1 x_{1,t} + \beta_{2}x_{2,t} + \beta_{3}x_{2,t}^2 + \beta_{4}x_{2,t}^3 + \beta_{5}x_{2,t}^4 + \beta_6 x_{3,t} + \beta_{7}\sin(x_{4,t}) + \beta_{8}\cos(x_{4,t}).
$$

We need to set priors on $\alpha$, $\beta_i$ for all $i$ and since JAGS works with precision instead of variance in the normal distribution, also on $\tau = 1 / \sigma^2$. We use the following priors on  $\alpha$, $\beta_i$:

$$
   \beta_i \sim Normal (0, 0.01) \text{ for all } i \in \{0, 8\} \\
    \alpha \sim Normal (0, 0.01),
$$

where 0.01 is the precision of the parameters (not the variance). As for $\tau$, we use a distribution that is often used in literature as a non informative prior on precision:

$$
    \tau \sim Gamma (0.01, 0.01).
$$

We write the model described above as a string that we feed to JAGS with the data to fit our model. 

```{r}
model_string2 <- "model {
  # Priors for the coefficients
  beta0 ~ dnorm(0, 0.01)
  beta1 ~ dnorm(0, 0.01)
  beta2 ~ dnorm(0, 0.01)
  beta3 ~ dnorm(0, 0.01)
  beta4 ~ dnorm(0, 0.01)
  beta5 ~ dnorm(0, 0.01)
  beta6 ~ dnorm(0, 0.01)
  beta7 ~ dnorm(0, 0.01)
  beta8 ~ dnorm(0, 0.01)
  alpha ~ dnorm(0, 0.01)
  
  # Prior for the precision (inverse of variance)
  tau ~ dgamma(0.01, 0.01)
  sigma <- 1 / sqrt(tau)
  
  # Initial value for y[1]
  y[1] ~ dnorm(0, 0.01)
  
  # Likelihood
  for (t in 2:N) {
    y[t] ~ dnorm(mu[t], tau)
    mu[t] <- beta0 + alpha * y[t-1] +                                               # intercept and y[t-1]
             beta1 * x1[t] +                                                        # temp x1
             beta2 * x2[t] + beta3 * x2[t]^2 + beta4 * x2[t]^3 + beta5 * x2[t]^4 +  # time of day x2
             beta6 * x3[t] +                                                        # weekend x3 
             beta7 * sin(x4[t]) + beta8 * cos(x4[t])                                # time of year x4
             }
  
}

"
```

We define a function that sets a seed for the initial values for the chains, for reproducibility. 

```{r}
# Define the initial values function
inits <- function(chain) {
  set.seed(12 + chain) # Different seed for each chain
  list(
    .RNG.name ="base::Mersenne-Twister",
    .RNG.seed = 12 + chain
  )
}
```

We fit the model above using 3 chains, a burn-in of 11000 and simulate a posterior sample of size 15000.

```{r}
set.seed(12)

# Data for the model
datalist <- list(N = length(y), x1 = x1, x2 = x2, x3 = x3, x4 = x4, y = y)

# Compile and run with 3 chains, burn-in of 9000 and total sample size of 13000
model <- jags.model(
  file = textConnection(model_string2),
  data = datalist,
  inits = list(inits(4), inits(5), inits(3)), # Seed for initial values for each chain
  n.chains = 3
)
update(model, n.iter = 11000)

Nrep = 15000

posterior_sample <- coda.samples(
  model,
  variable.names = c("tau", "beta0", "beta1", "beta2", "beta3", "beta4", 
                     "beta5", "beta6", "beta7", "beta8", "alpha"),
  n.iter = Nrep
)
```

To check for convergence of chains, we use the Gelman–Rubin convergence diagnostic. The Gelman-Rubin diagnostic compares the variance between multiple chains (inter-chain variance) to the variance within each chain (intra-chain variance). The basic idea is to run multiple MCMC chains and then evaluate whether these chains have converged to the same distribution. Mathematically, it can be expressed as:

$$
\hat{R}=\frac{\frac{L-1}{L}W + \frac{1}{L}B}{W}
$$

where $L$ is the total simulated sample size minus the burn in, $B$ is between chain variance and $W$ is the average within-chain variance. We want the $\hat{R}$ (or psrf) values to be close to 1, generally taking values under 1.2 or more strictly, under 1.1 to indicate convergence. We print the Gelman–Rubin convergence diagnostic below for each variable. 

```{r}
gelman.diag(posterior_sample)
```

We can see that each individual point estimate, as well as the multivariate psrf is under 1.2, so this confirms convergence. 

We see the summary of the posterior samples below. 

```{r}
summary(posterior_sample)
```

Next, we take the posterior means of the regression parameters as estimated by the model and calculate $\hat{\mu_t}$, which is the mean of the distribution of $y_t$ at time $t$, as estimated by our model. We can then plot the means against the true values of $y_t$ to see how well the model fits the data. 

```{r}
stat <- as.vector(unlist(summary(posterior_sample)[1])[1:11])
mu <- vector(length = length(y))
mu[1] <- y[1]

# Compare fitted mu with true values of y_t 
for (t in 2:length(y)) {
  mu[t] <- stat[2] + stat[1] * y[t-1] +                                                
             stat[3] * x1[t] +                                                         
             stat[4] * x2[t] + stat[5] * x2[t]^2 + stat[6] * x2[t]^3 + stat[7] * x2[t]^4 +   
             stat[8] * x3[t] +                                                        
             stat[9] * sin(x4[t]) + stat[10] * cos(x4[t])                     
}

# Create a time variable
t <- 1:length(y)

# Create a data frame
data <- data.frame(
  t = t,
  y = y,
  mu = mu
)

# Load ggplot2
library(ggplot2)

# Create the scatter plot
ggplot(data, aes(x = mu, y = y)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Fitted Values",
       x = "Fitted Values (mu)",
       y = "Actual Values (y)") +
  theme_minimal()
```

We can see that the model fits the data well, as the means agree with the true values.

Now we want to find the predicted values of y for the test set. This allows us to assess the model performance using performance metrics like MSE.

```{r}
set.seed(123)
# Extract the test data vectors
x1_test <- cluster1_data_test$temp
x2_test <- cluster1_data_test$tod
x3_test <- as.integer(cluster1_data_test$weekend)
x4_test <- cluster1_data_test$toy
y_test <- cluster1_data_test$Cluster.1
N_test <- length(y_test)

# Initialize a vector to store the predicted values
predicted_y <- numeric(N_test)

# Sigma is 1/sqrt(tau), where tau is the precision
sigma <- 1 / sqrt(stat[11])

# Make predictions using the posterior samples
mu_test <- numeric(N_test)
mu_test[1] <- y_test[1]
predicted_y[1] <- y_test[1]  # Initialize with the first observed value

for (t in 2:N_test) {
  
    mu_test[t] <- stat[2] + stat[1] * predicted_y[t-1] +                                                
             stat[3] * x1_test[t] +                                                         
             stat[4] * x2_test[t] + stat[5] * x2_test[t]^2 + stat[6] * x2_test[t]^3+ stat[7]*x2_test[t]^4+   
             stat[8] * x3_test[t] +                                                        
             stat[9] * sin(x4_test[t]) + stat[10] * cos(x4_test[t]) 
    
    predicted_y[t] <- rnorm(1, mean = mu_test[t], sd = sigma)
}

```

We plot the predicted vs. fitted values:

```{r}
# Create a data frame
t <- 1:length(y_test)

data_test <- data.frame(
  t = t,
  y = y_test,
  predicted_val = predicted_y
)

# Create the scatter plot
ggplot(data_test, aes(x = predicted_val, y = y)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Fitted Values",
       x = "Predicted y",
       y = "y") +
  theme_minimal()
```

Calculate MSE:

```{r}
# Calculate Mean Squared Error (MSE)
mse <- mean((predicted_y - y_test)^2)

# Print the results
cat("MSE:", mse, "\n")
```

Repeat for cluster 2:

```{r}
# use cluster 2 from dataset
cluster2_data <- aggregated_data[, -c(1,3,4,5,6)]

# take first 80% of the data
num_rows <- nrow(cluster2_data)
num_rows_80_percent <- floor(0.8 * num_rows)
cluster2_data_train <- cluster2_data[1:num_rows_80_percent, ]
cluster2_data_test <- cluster2_data[(num_rows_80_percent + 1):num_rows, ]

# define the dependent variable and teh explanatory variables
y <- cluster2_data_train$Cluster.2 
x4 <- cluster2_data_train$toy        # time of year
x3 <- cluster2_data_train$weekend    # weekend
x1 <- cluster2_data_train$temp       # temperature
x2 <- cluster2_data_train$tod        # time of day
```

Fit model with burn in 11000 and posterior sample size of 15000. 

```{r}
set.seed(12)

# Data for the model
datalist <- list(N = length(y), x1 = x1, x2 = x2, x3 = x3, x4 = x4, y = y)

# Compile and run with 3 chains, burn-in of 9000 and total sample size of 13000
model <- jags.model(
  file = textConnection(model_string2),
  data = datalist,
  inits = list(inits(4), inits(5), inits(3)), # Seed for initial values for each chain
  n.chains = 3
)
update(model, n.iter = 11000)

Nrep = 15000

posterior_sample2 <- coda.samples(
  model,
  variable.names = c("tau", "beta0", "beta1", "beta2", "beta3", "beta4", 
                     "beta5", "beta6", "beta7", "beta8", "alpha"),
  n.iter = Nrep
)
```


Check convergence: 

```{r}
gelman.diag(posterior_sample2)
```

We can see that each individual point estimate, as well as the multivariate psrf is under 1.2, so this confirms convergence. 

We print the summary of the posterior samples below. 

```{r}
summary(posterior_sample2)
```

Next, we take the posterior means of the regression parameters as estimated by the model and calculate $\hat{\mu_t}$, which is the mean of the distribution of $y_t$ at time $t$, as estimated by our model. We can then plot the means against the true values of $y_t$ to see how well the model fits the data. 

```{r}
stat <- as.vector(unlist(summary(posterior_sample2)[1])[1:11])
mu <- vector(length = length(y))
mu[1] <- y[1]

# Compare fitted mu with true values of y_t 
for (t in 2:length(y)) {
  mu[t] <- stat[2] + stat[1] * y[t-1] +                                                
             stat[3] * x1[t] +                                                         
             stat[4] * x2[t] + stat[5] * x2[t]^2 + stat[6] * x2[t]^3 + stat[7] * x2[t]^4 +   
             stat[8] * x3[t] +                                                        
             stat[9] * sin(x4[t]) + stat[10] * cos(x4[t])                     
}

# Create a time variable
t <- 1:length(y)

# Create a data frame
data <- data.frame(
  t = t,
  y = y,
  mu = mu
)

# Create the scatter plot
ggplot(data, aes(x = mu, y = y)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Fitted Values",
       x = "Fitted Values (mu)",
       y = "Actual Values (y)") +
  theme_minimal()
```

We can see that the model fits the data well, as the means agree with the true values.

Now we want to find the predicted values of y for the test set. This allows us to assess the model performance using performance metrics like MSE.

```{r}
set.seed(12)
# Extract the test data vectors
x1_test <- cluster2_data_test$temp
x2_test <- cluster2_data_test$tod
x3_test <- as.integer(cluster2_data_test$weekend)
x4_test <- cluster2_data_test$toy
y_test <- cluster2_data_test$Cluster.2
N_test <- length(y_test)

# Initialize a vector to store the predicted values
predicted_y <- numeric(N_test)

# Sigma is 1/sqrt(tau), where tau is the precision
sigma <- 1 / sqrt(stat[11])

# Make predictions using the posterior samples
mu_test <- numeric(N_test)
mu_test[1] <- y_test[1]
predicted_y[1] <- y_test[1]  # Initialize with the first observed value

for (t in 2:N_test) {
  
    mu_test[t] <- stat[2] + stat[1] * predicted_y[t-1] +                                                
             stat[3] * x1_test[t] +                                                         
             stat[4] * x2_test[t] + stat[5] * x2_test[t]^2 + stat[6] * x2_test[t]^3 + stat[7]*x2_test[t]^4+   
             stat[8] * x3_test[t] +                                                        
             stat[9] * sin(x4_test[t]) + stat[10] * cos(x4_test[t]) 
    
    predicted_y[t] <- rnorm(1, mean = mu_test[t], sd = sigma)
}

```

We plot the predicted vs. fitted values:

```{r}
# Create a data frame
t <- 1:length(y_test)

data_test <- data.frame(
  t = t,
  y = y_test,
  predicted_val = predicted_y
)

# Create the scatter plot
ggplot(data_test, aes(x = predicted_val, y = y)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Fitted Values",
       x = "Predicted y",
       y = "y") +
  theme_minimal()
```

Calculate MSE:

```{r}
# Calculate Mean Squared Error (MSE)
mse <- mean((predicted_y - y_test)^2)

# Print the results
cat("MSE:", mse, "\n")
```


Repeat for cluster 3:

```{r}
# use cluster 3 from dataset
cluster3_data <- aggregated_data[, -c(1,2,4,5,6)]

# take first 80% of the data
num_rows <- nrow(cluster3_data)
num_rows_80_percent <- floor(0.8 * num_rows)
cluster3_data_train <- cluster3_data[1:num_rows_80_percent, ]
cluster3_data_test <- cluster3_data[(num_rows_80_percent + 1):num_rows, ]

# define the dependent variable and the explanatory variables
y <- cluster3_data_train$Cluster.3 
x4 <- cluster3_data_train$toy        # time of year
x3 <- cluster3_data_train$weekend    # weekend
x1 <- cluster3_data_train$temp       # temperature
x2 <- cluster3_data_train$tod        # time of day
```

Fit model with burn in 12000 and posterior sample size of 16000. 

```{r}
set.seed(12)

# Data for the model
datalist <- list(N = length(y), x1 = x1, x2 = x2, x3 = x3, x4 = x4, y = y)

# Compile and run with 3 chains, burn-in of 9000 and total sample size of 13000
model <- jags.model(
  file = textConnection(model_string2),
  data = datalist,
  inits = list(inits(4), inits(5), inits(3)), # Seed for initial values for each chain
  n.chains = 3
)
update(model, n.iter = 12000)

Nrep = 16000

posterior_sample3 <- coda.samples(
  model,
  variable.names = c("tau", "beta0", "beta1", "beta2", "beta3", "beta4", 
                     "beta5", "beta6", "beta7", "beta8", "alpha"),
  n.iter = Nrep
)
```

Check convergence: 

```{r}
gelman.diag(posterior_sample3)
```

We can see that each individual point estimate, as well as the multivariate psrf is under 1.2, so this confirms convergence. 

We print the summary of the posterior samples below. 

```{r}
summary(posterior_sample3)
```

Next, we take the posterior means of the regression parameters as estimated by the model and calculate $\hat{\mu_t}$, which is the mean of the distribution of $y_t$ at time $t$, as estimated by our model. We can then plot the means against the true values of $y_t$ to see how well the model fits the data. 

```{r}
stat <- as.vector(unlist(summary(posterior_sample3)[1])[1:11])
mu <- vector(length = length(y))
mu[1] <- y[1]

# Compare fitted mu with true values of y_t 
for (t in 2:length(y)) {
  mu[t] <- stat[2] + stat[1] * y[t-1] +                                                
             stat[3] * x1[t] +                                                         
             stat[4] * x2[t] + stat[5] * x2[t]^2 + stat[6] * x2[t]^3 + stat[7] * x2[t]^4 +   
             stat[8] * x3[t] +                                                        
             stat[9] * sin(x4[t]) + stat[10] * cos(x4[t])                     
}

# Create a time variable
t <- 1:length(y)

# Create a data frame
data <- data.frame(
  t = t,
  y = y,
  mu = mu
)

# Create the scatter plot
ggplot(data, aes(x = mu, y = y)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Fitted Values",
       x = "Fitted Values (mu)",
       y = "Actual Values (y)") +
  theme_minimal()
```

We can see that the model fits the data well, as the means agree with the true values.

Now we want to find the predicted values of y for the test set. This allows us to assess the model performance using performance metrics like MSE.

```{r}
set.seed(12)
# Extract the test data vectors
x1_test <- cluster3_data_test$temp
x2_test <- cluster3_data_test$tod
x3_test <- as.integer(cluster3_data_test$weekend)
x4_test <- cluster3_data_test$toy
y_test <- cluster3_data_test$Cluster.3
N_test <- length(y_test)

# Initialize a vector to store the predicted values
predicted_y <- numeric(N_test)

# Sigma is 1/sqrt(tau), where tau is the precision
sigma <- 1 / sqrt(stat[11])

# Make predictions using the posterior samples
mu_test <- numeric(N_test)
mu_test[1] <- y_test[1]
predicted_y[1] <- y_test[1]  # Initialize with the first observed value

for (t in 2:N_test) {
  
    mu_test[t] <- stat[2] + stat[1] * predicted_y[t-1] +                                                
             stat[3] * x1_test[t] +                                                         
             stat[4] * x2_test[t] + stat[5] * x2_test[t]^2 + stat[6] * x2_test[t]^3 + stat[7]*x2_test[t]^4+   
             stat[8] * x3_test[t] +                                                        
             stat[9] * sin(x4_test[t]) + stat[10] * cos(x4_test[t]) 
    
    predicted_y[t] <- rnorm(1, mean = mu_test[t], sd = sigma)
}

```

We plot the predicted vs. fitted values:

```{r}
# Create a data frame
t <- 1:length(y_test)

data_test <- data.frame(
  t = t,
  y = y_test,
  predicted_val = predicted_y
)

# Create the scatter plot
ggplot(data_test, aes(x = predicted_val, y = y)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Fitted Values",
       x = "Predicted y",
       y = "y") +
  theme_minimal()
```


Calculate MSE:

```{r}
# Calculate Mean Squared Error (MSE)
mse <- mean((predicted_y - y_test)^2)

# Print the results
cat("MSE:", mse, "\n")
```


Repeat for cluster 4:


```{r}
# use cluster 4 from dataset
cluster4_data <- aggregated_data[, -c(1,2,3,5,6)]

# take first 80% of the data
num_rows <- nrow(cluster4_data)
num_rows_80_percent <- floor(0.8 * num_rows)
cluster4_data_train <- cluster4_data[1:num_rows_80_percent, ]
cluster4_data_test <- cluster4_data[(num_rows_80_percent + 1):num_rows, ]

# define the dependent variable and the explanatory variables
y <- cluster4_data_train$Cluster.4 
x4 <- cluster4_data_train$toy        # time of year
x3 <- cluster4_data_train$weekend    # weekend
x1 <- cluster4_data_train$temp       # temperature
x2 <- cluster4_data_train$tod        # time of day
```

Fit model with burn in 11000 and posterior sample size of 15000. 

```{r}
set.seed(12)

# Data for the model
datalist <- list(N = length(y), x1 = x1, x2 = x2, x3 = x3, x4 = x4, y = y)

# Compile and run with 3 chains, burn-in of 9000 and total sample size of 13000
model <- jags.model(
  file = textConnection(model_string2),
  data = datalist,
  inits = list(inits(4), inits(5), inits(3)), # Seed for initial values for each chain
  n.chains = 3
)
update(model, n.iter = 11000)

Nrep = 15000

posterior_sample4 <- coda.samples(
  model,
  variable.names = c("tau", "beta0", "beta1", "beta2", "beta3", "beta4", 
                     "beta5", "beta6", "beta7", "beta8", "alpha"),
  n.iter = Nrep
)
```

Check convergence: 

```{r}
gelman.diag(posterior_sample4)
```

We can see that each individual point estimate, as well as the multivariate psrf is under 1.2, so this confirms convergence. 

We print the summary of the posterior samples below. 

```{r}
summary(posterior_sample4)
```

Next, we take the posterior means of the regression parameters as estimated by the model and calculate $\hat{\mu_t}$, which is the mean of the distribution of $y_t$ at time $t$, as estimated by our model. We can then plot the means against the true values of $y_t$ to see how well the model fits the data. 

```{r}
stat <- as.vector(unlist(summary(posterior_sample4)[1])[1:11])
mu <- vector(length = length(y))
mu[1] <- y[1]

# Compare fitted mu with true values of y_t 
for (t in 2:length(y)) {
  mu[t] <- stat[2] + stat[1] * y[t-1] +                                                
             stat[3] * x1[t] +                                                         
             stat[4] * x2[t] + stat[5] * x2[t]^2 + stat[6] * x2[t]^3 + stat[7] * x2[t]^4 +   
             stat[8] * x3[t] +                                                        
             stat[9] * sin(x4[t]) + stat[10] * cos(x4[t])                     
}

# Create a time variable
t <- 1:length(y)

# Create a data frame
data <- data.frame(
  t = t,
  y = y,
  mu = mu
)

# Create the scatter plot
ggplot(data, aes(x = mu, y = y)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Fitted Values",
       x = "Fitted Values (mu)",
       y = "Actual Values (y)") +
  theme_minimal()
```

We can see that the model fits the data well, as the means agree with the true values.

Now we want to find the predicted values of y for the test set. This allows us to assess the model performance using performance metrics like MSE.

```{r}
set.seed(12)
# Extract the test data vectors
x1_test <- cluster4_data_test$temp
x2_test <- cluster4_data_test$tod
x3_test <- as.integer(cluster4_data_test$weekend)
x4_test <- cluster4_data_test$toy
y_test <- cluster4_data_test$Cluster.4
N_test <- length(y_test)

# Initialize a vector to store the predicted values
predicted_y <- numeric(N_test)

# Sigma is 1/sqrt(tau), where tau is the precision
sigma <- 1 / sqrt(stat[11])

# Make predictions using the posterior samples
mu_test <- numeric(N_test)
mu_test[1] <- y_test[1]
predicted_y[1] <- y_test[1]  # Initialize with the first observed value

for (t in 2:N_test) {
  
    mu_test[t] <- stat[2] + stat[1] * predicted_y[t-1] +                                                
             stat[3] * x1_test[t] +                                                         
             stat[4] * x2_test[t] + stat[5] * x2_test[t]^2 + stat[6] * x2_test[t]^3 + stat[7]*x2_test[t]^4+   
             stat[8] * x3_test[t] +                                                        
             stat[9] * sin(x4_test[t]) + stat[10] * cos(x4_test[t]) 
    
    predicted_y[t] <- rnorm(1, mean = mu_test[t], sd = sigma)
}

```

We plot the predicted vs. fitted values:

```{r}
# Create a data frame
t <- 1:length(y_test)

data_test <- data.frame(
  t = t,
  y = y_test,
  predicted_val = predicted_y
)

# Create the scatter plot
ggplot(data_test, aes(x = predicted_val, y = y)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Fitted Values",
       x = "Predicted y",
       y = "y") +
  theme_minimal()
```

Calculate MSE:

```{r}
# Calculate Mean Squared Error (MSE)
mse <- mean((predicted_y - y_test)^2)

# Print the results
cat("MSE:", mse, "\n")
```


Repeat for cluster 5:

```{r}
# use cluster 5 from dataset
cluster5_data <- aggregated_data[, -c(1,2,3,4,6)]

# take first 80% of the data
num_rows <- nrow(cluster5_data)
num_rows_80_percent <- floor(0.8 * num_rows)
cluster5_data_train <- cluster5_data[1:num_rows_80_percent, ]
cluster5_data_test <- cluster5_data[(num_rows_80_percent + 1):num_rows, ]

# define the dependent variable and the explanatory variables
y <- cluster5_data_train$Cluster.5 
x4 <- cluster5_data_train$toy        # time of year
x3 <- cluster5_data_train$weekend    # weekend
x1 <- cluster5_data_train$temp       # temperature
x2 <- cluster5_data_train$tod        # time of day
```

Fit model with burn in 12000 and posterior sample size of 16000. 

```{r}
set.seed(12)

# Data for the model
datalist <- list(N = length(y), x1 = x1, x2 = x2, x3 = x3, x4 = x4, y = y)

# Compile and run with 3 chains, burn-in of 9000 and total sample size of 13000
model <- jags.model(
  file = textConnection(model_string2),
  data = datalist,
  inits = list(inits(4), inits(5), inits(3)), # Seed for initial values for each chain
  n.chains = 3
)
update(model, n.iter = 12000)

Nrep = 16000

posterior_sample5 <- coda.samples(
  model,
  variable.names = c("tau", "beta0", "beta1", "beta2", "beta3", "beta4", 
                     "beta5", "beta6", "beta7", "beta8", "alpha"),
  n.iter = Nrep
)
```

Check convergence:  

```{r}
gelman.diag(posterior_sample5)
```

In this case, the distributions of the variables $\beta_2, \beta_3, \beta_4$ and $\beta_5$ failed to converge. We tried with different initial values and different burn-in/posterior sample sizes, but this remained the case for all tried values, so we just conclude that for cluster 5, the model failed to converge. However, we can still try and use the posterior means in our model to predict for the test set. 

We print the summary of the posterior samples below. 

```{r}
summary(posterior_sample5)
```

Next, we take the posterior means of the regression parameters as estimated by the model and calculate $\hat{\mu_t}$, which is the mean of the distribution of $y_t$ at time $t$, as estimated by our model. We can then plot the means against the true values of $y_t$ to see how well the model fits the data. 

```{r}
stat <- as.vector(unlist(summary(posterior_sample5)[1])[1:11])
mu <- vector(length = length(y))
mu[1] <- y[1]

# Compare fitted mu with true values of y_t 
for (t in 2:length(y)) {
  mu[t] <- stat[2] + stat[1] * y[t-1] +                                                
             stat[3] * x1[t] +                                                         
             stat[4] * x2[t] + stat[5] * x2[t]^2 + stat[6] * x2[t]^3 + stat[7] * x2[t]^4 +   
             stat[8] * x3[t] +                                                        
             stat[9] * sin(x4[t]) + stat[10] * cos(x4[t])                     
}

# Create a time variable
t <- 1:length(y)

# Create a data frame
data <- data.frame(
  t = t,
  y = y,
  mu = mu
)

# Create the scatter plot
ggplot(data, aes(x = mu, y = y)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Fitted Values",
       x = "Fitted Values (mu)",
       y = "Actual Values (y)") +
  theme_minimal()
```

We can see that the model fits the data well, as the means agree with the true values.

Now we want to find the predicted values of y for the test set. This allows us to assess the model performance using performance metrics like MSE.

```{r}
set.seed(12)
# Extract the test data vectors
x1_test <- cluster5_data_test$temp
x2_test <- cluster5_data_test$tod
x3_test <- as.integer(cluster5_data_test$weekend)
x4_test <- cluster5_data_test$toy
y_test <- cluster5_data_test$Cluster.5
N_test <- length(y_test)

# Initialize a vector to store the predicted values
predicted_y <- numeric(N_test)

# Sigma is 1/sqrt(tau), where tau is the precision
sigma <- 1 / sqrt(stat[11])

# Make predictions using the posterior samples
mu_test <- numeric(N_test)
mu_test[1] <- y_test[1]
predicted_y[1] <- y_test[1]  # Initialize with the first observed value

for (t in 2:N_test) {
  
    mu_test[t] <- stat[2] + stat[1] * predicted_y[t-1] +                                                
             stat[3] * x1_test[t] +                                                         
             stat[4] * x2_test[t] + stat[5] * x2_test[t]^2 + stat[6] * x2_test[t]^3 + stat[7]*x2_test[t]^4+   
             stat[8] * x3_test[t] +                                                        
             stat[9] * sin(x4_test[t]) + stat[10] * cos(x4_test[t]) 
    
    predicted_y[t] <- rnorm(1, mean = mu_test[t], sd = sigma)
}

```

We plot the predicted vs. fitted values:

```{r}
# Create a data frame
t <- 1:length(y_test)

data_test <- data.frame(
  t = t,
  y = y_test,
  predicted_val = predicted_y
)

# Create the scatter plot
ggplot(data_test, aes(x = predicted_val, y = y)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Fitted Values",
       x = "Predicted y",
       y = "y") +
  theme_minimal()
```

Calculate MSE:

```{r}
# Calculate Mean Squared Error (MSE)
mse <- mean((predicted_y - y_test)^2)

# Print the results
cat("MSE:", mse, "\n")
```

So even though convergence failed, the model predicts the test data well. 

Repeat for cluster 6:

```{r}
# use cluster 6 from dataset
cluster6_data <- aggregated_data[, -c(1,2,3,4,5)]

# take first 80% of the data
num_rows <- nrow(cluster6_data)
num_rows_80_percent <- floor(0.8 * num_rows)
cluster6_data_train <- cluster6_data[1:num_rows_80_percent, ]
cluster6_data_test <- cluster6_data[(num_rows_80_percent + 1):num_rows, ]

# define the dependent variable and the explanatory variables
y <- cluster6_data_train$Cluster.6 
x4 <- cluster6_data_train$toy        # time of year
x3 <- cluster6_data_train$weekend    # weekend
x1 <- cluster6_data_train$temp       # temperature
x2 <- cluster6_data_train$tod        # time of day
```

Fit model with burn in 12000 and posterior sample size of 16000. 

```{r}
set.seed(12)

# Data for the model
datalist <- list(N = length(y), x1 = x1, x2 = x2, x3 = x3, x4 = x4, y = y)

# Compile and run with 3 chains, burn-in of 11000 and total sample size of 15000
model <- jags.model(
  file = textConnection(model_string2),
  data = datalist,
  inits = list(inits(4), inits(5), inits(3)), # Seed for initial values for each chain
  n.chains = 3
)
update(model, n.iter = 12000)

Nrep = 16000

posterior_sample6 <- coda.samples(
  model,
  variable.names = c("tau", "beta0", "beta1", "beta2", "beta3", "beta4", 
                     "beta5", "beta6", "beta7", "beta8", "alpha"),
  n.iter = Nrep
)
```

Check convergence:  

```{r}
gelman.diag(posterior_sample6)
```

Again, we see that the distributions of the variables $\beta_2, \beta_3, \beta_4$ and $\beta_5$ failed to converge. We tried with different initial values and different burn-in/posterior sample sizes, but this remained the case for all tested values, so we conclude that the model also failed to converge for cluster 6. We can still try and use the posterior means in our model to predict for the test set. 

We print the summary of the posterior samples below. 

```{r}
summary(posterior_sample6)
```

Next, we take the posterior means of the regression parameters as estimated by the model and calculate $\hat{\mu_t}$, which is the mean of the distribution of $y_t$ at time $t$, as estimated by our model. We can then plot the means against the true values of $y_t$ to see how well the model fits the data. 

```{r}
stat <- as.vector(unlist(summary(posterior_sample6)[1])[1:11])
mu <- vector(length = length(y))
mu[1] <- y[1]

# Compare fitted mu with true values of y_t 
for (t in 2:length(y)) {
  mu[t] <- stat[2] + stat[1] * y[t-1] +                                                
             stat[3] * x1[t] +                                                         
             stat[4] * x2[t] + stat[5] * x2[t]^2 + stat[6] * x2[t]^3 + stat[7] * x2[t]^4 +   
             stat[8] * x3[t] +                                                        
             stat[9] * sin(x4[t]) + stat[10] * cos(x4[t])                     
}

# Create a time variable
t <- 1:length(y)

# Create a data frame
data <- data.frame(
  t = t,
  y = y,
  mu = mu
)

# Create the scatter plot
ggplot(data, aes(x = mu, y = y)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Fitted Values",
       x = "Fitted Values (mu)",
       y = "Actual Values (y)") +
  theme_minimal()
```

We can see that the model fits the data well, as the means agree with the true values.

Now we want to find the predicted values of y for the test set. This allows us to assess the model performance using performance metrics like MSE.

```{r}
set.seed(12)
# Extract the test data vectors
x1_test <- cluster6_data_test$temp
x2_test <- cluster6_data_test$tod
x3_test <- as.integer(cluster6_data_test$weekend)
x4_test <- cluster6_data_test$toy
y_test <- cluster6_data_test$Cluster.6
N_test <- length(y_test)

# Initialize a vector to store the predicted values
predicted_y <- numeric(N_test)

# Sigma is 1/sqrt(tau), where tau is the precision
sigma <- 1 / sqrt(stat[11])

# Make predictions using the posterior samples
mu_test <- numeric(N_test)
mu_test[1] <- y_test[1]
predicted_y[1] <- y_test[1]  # Initialize with the first observed value

for (t in 2:N_test) {
  
    mu_test[t] <- stat[2] + stat[1] * predicted_y[t-1] +                                                
             stat[3] * x1_test[t] +                                                         
             stat[4] * x2_test[t] + stat[5] * x2_test[t]^2 + stat[6] * x2_test[t]^3 + stat[7]*x2_test[t]^4+   
             stat[8] * x3_test[t] +                                                        
             stat[9] * sin(x4_test[t]) + stat[10] * cos(x4_test[t]) 
    
    predicted_y[t] <- rnorm(1, mean = mu_test[t], sd = sigma)
}

```

We plot the predicted vs. fitted values:

```{r}
# Create a data frame
t <- 1:length(y_test)

data_test <- data.frame(
  t = t,
  y = y_test,
  predicted_val = predicted_y
)

# Create the scatter plot
ggplot(data_test, aes(x = predicted_val, y = y)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Fitted Values",
       x = "Predicted y",
       y = "y") +
  theme_minimal()
```

Calculate MSE:

```{r}
# Calculate Mean Squared Error (MSE)
mse <- mean((predicted_y - y_test)^2)

# Print the results
cat("MSE:", mse, "\n")
```

So even though convergence failed, the model predicts the test data well. 



## Conclusion

In this study, we successfully developed and tested a Bayesian time series regression model to forecast household electricity demand. Our exploration began with a detailed exploratory data analysis (EDA), followed by effective data aggregation strategies, and culminated in the adoption of Bayesian methods for prediction, which were rigorously evaluated against standard performance metrics. We summarise our work as follows.

1. **Exploratory Data Analysis (EDA):** We commenced with a comprehensive analysis to understand the distribution and variability of electricity demand. This phase involved identifying key patterns, trends, and outliers, which facilitated the selection of relevant features for further modelling.

2. **Data Aggregation:** By clustering households based on their daily demand profiles, we reduced the dataset's dimensionality. This enabled more focused and efficient modelling, making the process manageable despite the dataset's large size.

3. **Model Development:** We implemented a Bayesian time series regression model that integrates non-linear transformations and accounts for temporal dependencies. This model structure was chosen based on its suitability for the dataset characteristics highlighted in the EDA phase.

4. **Model Evaluation:** The predictive accuracy of the model was assessed using metrics such as Mean Squared Error (MSE). Comparisons were made between our Bayesian approach and traditional linear regression models to underline the strengths of our methodology.

Our findings reveal that Bayesian models, equipped with non-linear transformations and temporal dynamics, provide a robust framework for capturing complex patterns in electricity demand. The performance of the model, highlighted by its ability to accommodate seasonal variations and daily fluctuations, demonstrates its potential utility in practical energy management scenarios. Notably, the comparison of Bayesian approaches with traditional linear regression models showcased the former’s superior capability in handling large, noisy datasets typical in smart metering environments.

Looking forward, this research opens several avenues for further investigation. Future studies could explore the integration of additional predictive factors such as demographic and economic indicators to enhance the model's accuracy. Additionally, the application of more sophisticated statistical techniques could be considered to improve the adaptability and predictive performance of the models under varying conditions.

The implications of this research are significant, offering a promising direction for energy policymakers and utility companies to enhance their demand forecasting capabilities, ultimately leading to more efficient and sustainable energy management practices.

